{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from collections import deque\n",
    "from dqn_agent_PER import Agent\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_PATH = \"environment-MAC/en.app\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialise customised Banana Collecter environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_env(ENV_PATH):\n",
    "    # env = UnityEnvironment(file_name=ENV_PATH)\n",
    "    env = UE(base_port=5004,file_name=ENV_PATH, seed=1, side_channels=[])\n",
    "    env.step()\n",
    "    # in this project, we are only using one agent, so we will only work on the first `brain` in the environmet\n",
    "    # get the default brain\n",
    "    # brain_name = env.brain_names[0]\n",
    "    brain_name = list(env.behavior_specs.keys())[0]\n",
    "    # brain = env.brains[brain_name]\n",
    "    brain = env.behavior_specs[brain_name]\n",
    "    return env, brain, brain_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = UE(file_name=ENV_PATH, seed=1, side_channels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = env.behavior_specs.keys()\n",
    "# print(list(test)[0])\n",
    "# for t in test:\n",
    "#     print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brain_name = list(env.behavior_specs.keys())[0]\n",
    "# print(brain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brain = env.behavior_specs[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0617 03:24:38.817811000 8664200704 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n"
     ]
    }
   ],
   "source": [
    "env, brain, brain_name = initialise_env(ENV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(brain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先让他走几步直到需要交互\n",
    "# env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialise the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BehaviorSpec(observation_specs=[ObservationSpec(shape=(128, 128, 3), dimension_property=(<DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.NONE: 1>), observation_type=<ObservationType.DEFAULT: 0>, name='CameraSensor'), ObservationSpec(shape=(147,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor-1'), ObservationSpec(shape=(147,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor-2'), ObservationSpec(shape=(147,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor-3'), ObservationSpec(shape=(0,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor'), ObservationSpec(shape=(4,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size4')], action_spec=ActionSpec(continuous_size=0, discrete_branches=(5,)))\n"
     ]
    }
   ],
   "source": [
    "spec = env.behavior_specs['My Behavior?team=0']\n",
    "print(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "#env_info = env.reset(train_mode=True)[brain_name]\n",
    "env_info = env.reset()\n",
    "#action_size = brain.vector_action_space_size\n",
    "#state_size = len(env_info.vector_observations[0])\n",
    "#agent = Agent(state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(env_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = list(brain.action_spec)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change\n",
    "# decision_steps.obs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ObservationSpec(shape=(128, 128, 3), dimension_property=(<DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.NONE: 1>), observation_type=<ObservationType.DEFAULT: 0>, name='CameraSensor'), ObservationSpec(shape=(147,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor-1'), ObservationSpec(shape=(147,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor-2'), ObservationSpec(shape=(147,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor-3'), ObservationSpec(shape=(0,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor'), ObservationSpec(shape=(4,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size4')]\n"
     ]
    }
   ],
   "source": [
    "print(brain.observation_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = len(brain.observation_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ObservationSpec(shape=(128, 128, 3), dimension_property=(<DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.NONE: 1>), observation_type=<ObservationType.DEFAULT: 0>, name='CameraSensor'),\n",
       " ObservationSpec(shape=(147,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor-1'),\n",
       " ObservationSpec(shape=(147,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor-2'),\n",
       " ObservationSpec(shape=(147,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor-3'),\n",
       " ObservationSpec(shape=(0,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor'),\n",
       " ObservationSpec(shape=(4,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size4')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.observation_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_size = len(env_info.vector_observations[0])\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(x):\n",
    "    \"\"\" plots first ncols images in a batch \"\"\"\n",
    "    x = x.squeeze(1).T\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 20))\n",
    "    ax.imshow(x, cmap=\"Greys\")\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms as transforms\n",
    "actions = []\n",
    "for act in range(5):\n",
    "    actions.append(spec.action_spec.empty_action(1))\n",
    "    actions[act].add_discrete(np.int32([[act]]))\n",
    "stop = actions[0]\n",
    "forward = actions[1]\n",
    "backward = actions[2]\n",
    "turn_right = actions[3]\n",
    "turn_left = actions[4]\n",
    "def train_dqn(agent, n_episodes=2, max_t=100, eps_start=1.0, eps_end=0.1, eps_decay=0.99):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    rewards =0\n",
    "    reward =0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "#         state = np.moveaxis(decision_steps.obs[0], -1, 0)\n",
    "        ray_sensor_1 = decision_steps.obs[1]\n",
    "#         print(ray_sensor_1.shape)\n",
    "        ray_sensor_2 = decision_steps.obs[2]\n",
    "        ray_sensor_3 = decision_steps.obs[3]\n",
    "#         print(ray_sensor_3.shape)\n",
    "#         print(ray_sensor_2.shape)\n",
    "        state = np.concatenate((ray_sensor_1, ray_sensor_2, ray_sensor_3), axis=1)\n",
    "#         print(state)\n",
    "#         print(state.shape)\n",
    "#         print(decision_steps.obs[2])\n",
    "#         print(decision_steps.obs[2].shape)\n",
    "        tracked_agent = -1\n",
    "        done = False\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                tracked_agent = decision_steps.agent_id[0]\n",
    "#             show_images(state)\n",
    "            action = agent.act(state, eps)\n",
    "            env.set_actions(brain_name, actions[action])\n",
    "            env.step()\n",
    "            decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "            \n",
    "#             next_state = np.moveaxis(decision_steps.obs[0], -1, 0)   # get the next state\n",
    "            ray_sensor_1 = decision_steps.obs[1]\n",
    "            ray_sensor_2 = decision_steps.obs[2]\n",
    "            ray_sensor_3 = decision_steps.obs[3]\n",
    "            next_state = np.concatenate((ray_sensor_1, ray_sensor_2, ray_sensor_3), axis=1)\n",
    "            if tracked_agent in decision_steps:# The agent requested a decision\n",
    "                reward = decision_steps[tracked_agent].reward  # get the reward\n",
    "                agent.step(state, action, reward, next_state, False)\n",
    "            if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "                rewards = terminal_steps[tracked_agent].reward# get the reward\n",
    "                agent.step(state, action, reward, next_state, True)\n",
    "                env.reset()\n",
    "                break\n",
    "            state = next_state\n",
    "#             print(reward)\n",
    "            score += reward\n",
    "            \n",
    "        \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        print(scores_window)\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, score), end=\"\")\n",
    "        if i_episode % 50 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, score))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            print('saved temporary learned weight')\n",
    "#         if np.mean(scores_window)>=500.0:\n",
    "#             print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "#             torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "#             print('agent done training')\n",
    "#             break\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([4.0], maxlen=100)\n",
      "Episode 1\tAverage Score: 4.00deque([4.0, 10.0], maxlen=100)\n",
      "Episode 2\tAverage Score: 10.00deque([4.0, 10.0, 7.0], maxlen=100)\n",
      "Episode 3\tAverage Score: 7.00deque([4.0, 10.0, 7.0, -2.0], maxlen=100)\n",
      "Episode 4\tAverage Score: -2.00deque([4.0, 10.0, 7.0, -2.0, -1.0], maxlen=100)\n",
      "Episode 5\tAverage Score: -1.00deque([4.0, 10.0, 7.0, -2.0, -1.0, 0.0], maxlen=100)\n",
      "Episode 6\tAverage Score: 0.00deque([4.0, 10.0, 7.0, -2.0, -1.0, 0.0, 0.0], maxlen=100)\n",
      "Episode 7\tAverage Score: 0.00deque([4.0, 10.0, 7.0, -2.0, -1.0, 0.0, 0.0, 0.0], maxlen=100)\n",
      "Episode 8\tAverage Score: 0.00deque([4.0, 10.0, 7.0, -2.0, -1.0, 0.0, 0.0, 0.0, 0.0], maxlen=100)\n",
      "Episode 9\tAverage Score: 0.00deque([4.0, 10.0, 7.0, -2.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0], maxlen=100)\n",
      "Episode 10\tAverage Score: 0.00"
     ]
    }
   ],
   "source": [
    "# if os.path.isfile('./checkpoint.pth'):\n",
    "#     # load the weights from file\n",
    "#     agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "    \n",
    "scores = train_dqn(agent, n_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Watch a trained agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def watch_banana_agent(agent, env, n_episodes=4, n_steps=300):\n",
    "\n",
    "                                   \n",
    "    \n",
    "#     for episode in range(n_episodes):\n",
    "        \n",
    "#         env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "#         state = env_info.vector_observations[0]            # get the current state\n",
    "#         score = 0                                          # initialize the score\n",
    "        \n",
    "#         for step in range(n_steps):\n",
    "\n",
    "#             action = agent.act(state)                 # select an action\n",
    "#             env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#             next_state = env_info.vector_observations[0]   # get the next state\n",
    "#             reward = env_info.rewards[0]                   # get the reward\n",
    "#             done = env_info.local_done[0]                  # see if episode has finished\n",
    "#             score += reward                                # update the score\n",
    "#             state = next_state                             # roll over the state to next time step\n",
    "#             if done:                                       # exit loop if episode finished\n",
    "#                 break\n",
    "\n",
    "#         print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch_banana_agent(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
